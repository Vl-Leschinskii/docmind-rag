# agents/smart_chunker.py (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import nltk
import traceback
from typing import List, Dict, Any

# –°–∫–∞—á–∏–≤–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã NLTK
try:
    nltk.download('punkt', quiet=True)
    from nltk.tokenize import sent_tokenize
except Exception as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ NLTK: {e}")
    def sent_tokenize(text):
        # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        return [s.strip() + '.' for s in text.split('.') if s.strip()]

class SmartChunkerAgent:
    """–ê–≥–µ–Ω—Ç –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
    
    def __init__(self, embedding_model="all-MiniLM-L6-v2", chunk_size=500, overlap=50):
        self.chunk_size = chunk_size
        self.overlap = overlap
        
        print(f"üì¶ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–µ—Ä–∞ —Å –º–æ–¥–µ–ª—å—é: {embedding_model}")
        
        try:
            self.embedder = SentenceTransformer(embedding_model)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {embedding_model}: {e}")
            print(f"üì¶ –ü—Ä–æ–±—É—é fallback –º–æ–¥–µ–ª—å: all-MiniLM-L6-v2")
            try:
                self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
                print(f"‚úÖ Fallback –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e2:
                print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e2}")
                raise
    
    def semantic_chunking(self, text: str) -> List[str]:
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
        try:
            if not text or not isinstance(text, str):
                print(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π —Ç–µ–∫—Å—Ç")
                return []
            
            if len(text) < self.chunk_size:
                return [text]
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            try:
                sentences = sent_tokenize(text)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: {e}")
                sentences = text.split('. ')
            
            print(f"üìä –ü–æ–ª—É—á–µ–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(sentences)}")
            
            chunks = []
            current_chunk = []
            current_size = 0
            
            for i, sentence in enumerate(sentences):
                sentence = sentence.strip()
                if not sentence:
                    continue
                    
                sentence_size = len(sentence)
                
                if current_size + sentence_size > self.chunk_size and current_chunk:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text)
                    print(f"  ‚ûï –ß–∞–Ω–∫ {len(chunks)}: {len(chunk_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                    
                    # –°–æ–∑–¥–∞–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
                    overlap_chunk = []
                    overlap_size = 0
                    for s in reversed(current_chunk):
                        if overlap_size + len(s) < self.overlap:
                            overlap_chunk.insert(0, s)
                            overlap_size += len(s)
                        else:
                            break
                    
                    current_chunk = overlap_chunk
                    current_size = overlap_size
                
                current_chunk.append(sentence)
                current_size += sentence_size
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            if current_chunk:
                chunk_text = ' '.join(current_chunk)
                chunks.append(chunk_text)
                print(f"  ‚ûï –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫ {len(chunks)}: {len(chunk_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            print(f"‚úÖ –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
            return chunks
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ semantic_chunking: {e}")
            traceback.print_exc()
            return [text]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–¥–∏–Ω —á–∞–Ω–∫ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    
    def split_by_semantics(self, text: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –≥—Ä–∞–Ω–∏—Ü–∞–º"""
        try:
            if not text or len(text) < 100:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
                return [text]
            
            sentences = sent_tokenize(text)
            if len(sentences) <= 1:
                return [text]
            
            print(f"üî¨ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ {len(sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π...")
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            try:
                embeddings = self.embedder.encode(sentences)
                print(f"   –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–ª—É—á–µ–Ω—ã: {embeddings.shape}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
                return self.semantic_chunking(text)
            
            # –ò—â–µ–º —Ç–æ—á–∫–∏ —Ä–∞–∑—Ä—ã–≤–∞
            breaks = [0]
            for i in range(1, len(embeddings)):
                similarity = cosine_similarity(
                    embeddings[i-1].reshape(1, -1),
                    embeddings[i].reshape(1, -1)
                )[0][0]
                
                if similarity < 0.6:  # –ü–æ—Ä–æ–≥ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä—ã–≤–∞
                    breaks.append(i)
                    print(f"   –†–∞–∑—Ä—ã–≤ –ø–æ—Å–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è {i} (—Å—Ö–æ–∂–µ—Å—Ç—å: {similarity:.3f})")
            
            breaks.append(len(sentences))
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
            chunks = []
            for i in range(len(breaks)-1):
                chunk = ' '.join(sentences[breaks[i]:breaks[i+1]])
                if chunk.strip():
                    chunks.append(chunk)
            
            print(f"‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–ª–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            return chunks if chunks else [text]
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ split_by_semantics: {e}")
            traceback.print_exc()
            return self.semantic_chunking(text)