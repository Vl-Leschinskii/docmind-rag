
import os
import json
from typing import List, Dict, Any, Optional

from agents.doc_parser import DocParserAgent
#from agents.smart_chunker import SmartChunkerAgent
from agents.simple_chunker import SimpleChunkerAgent


from agents.vector_agent import VectorAgent
from agents.answer_gpt import AnswerGPTAgent
from agents.validator import ValidatorAgent

class RAGOrchestrator:

    """–û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π RAG —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self, config_path: str = "config.yaml"):
        import yaml
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        self.config = config
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤
        self.agents = {
            'parser': DocParserAgent(),
            'chunker': SimpleChunkerAgent(chunk_size=50, overlap=5),
           #  'chunker': SmartChunkerAgent(
           #     embedding_model=config['embedding_model'],
           #     chunk_size=config['chunk_size'],
           #     overlap=config['overlap_size']
           # ),
            'vector': VectorAgent(
                embedding_model=config['embedding_model'],
                use_gpu=config['use_gpu'],
                batch_size=config['batch_size'],
                db_path=config['vector_db_path']
            ),
            'generator': AnswerGPTAgent(
                api_base=config['lm_studio_url'],
                model=config['generation_model'],
                temperature=config['temperature'],
                max_tokens=config['max_tokens']
            ),
            'validator': ValidatorAgent()
        }
        
        self.doc_structure = None
        self.is_indexed = False

def process_document(self, docx_path: str) -> Dict[str, Any]:
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    print(f"\nüìÑ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {docx_path}")
    
    try:
        # 1. –ü–∞—Ä—Å–∏–Ω–≥
        print("üîç –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
        self.doc_structure = self.agents['parser'].parse_with_hierarchy(docx_path)
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –≥–ª–∞–≤: {len(self.doc_structure['chapters'])}")
        
        # 2. –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
        print("‚úÇÔ∏è –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏...")
        chunks = []
        metadata = []
        
        for chapter_idx, chapter in enumerate(self.doc_structure['chapters']):
            print(f"  –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–ª–∞–≤—ã {chapter_idx + 1}: {chapter['title'][:50]}...")
            
            if chapter.get('content'):
                print(f"    –ö–æ–Ω—Ç–µ–Ω—Ç –≥–ª–∞–≤—ã: {len(chapter['content'])} —Å–∏–º–≤–æ–ª–æ–≤")
                chapter_chunks = self.agents['chunker'].split_by_semantics(chapter['content'])
                print(f"    –ü–æ–ª—É—á–µ–Ω–æ —á–∞–Ω–∫–æ–≤ –∏–∑ –≥–ª–∞–≤—ã: {len(chapter_chunks)}")
                
                chunks.extend(chapter_chunks)
                for chunk in chapter_chunks:
                    metadata.append({
                        'chapter_id': chapter['id'],
                        'chapter_title': chapter['title'],
                        'level': 1,
                        'type': 'chapter'
                    })
            
            for section_idx, section in enumerate(chapter.get('sections', [])):
                print(f"    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–¥–µ–ª–∞ {section_idx + 1}: {section['title'][:50]}...")
                
                if section.get('content'):
                    section_chunks = self.agents['chunker'].split_by_semantics(section['content'])
                    print(f"      –ü–æ–ª—É—á–µ–Ω–æ —á–∞–Ω–∫–æ–≤ –∏–∑ —Ä–∞–∑–¥–µ–ª–∞: {len(section_chunks)}")
                    
                    chunks.extend(section_chunks)
                    for chunk in section_chunks:
                        metadata.append({
                            'chapter_id': chapter['id'],
                            'chapter_title': chapter['title'],
                            'section_id': section['id'],
                            'section_title': section['title'],
                            'level': 2,
                            'type': 'section'
                        })
        
        print(f"üìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
        
        if len(chunks) == 0:
            print("‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –Ω–µ —Å–æ–∑–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞!")
            # –°–æ–∑–¥–∞–µ–º —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —á–∞–Ω–∫ –∏–∑ –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            all_text = ' '.join([chapter.get('content', '') for chapter in self.doc_structure['chapters']])
            if all_text:
                chunks = [all_text]
                metadata = [{'chapter_id': 'all', 'chapter_title': '–í–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç', 'level': 0, 'type': 'full'}]
                print(f"‚úÖ –°–æ–∑–¥–∞–Ω –æ–¥–∏–Ω –æ–±—â–∏–π —á–∞–Ω–∫ –∏–∑ {len(all_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # 3. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è
        print("üîó –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
        self.agents['vector'].create_index(chunks, metadata)
        self.is_indexed = True
        
        print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω. –ì–ª–∞–≤: {len(self.doc_structure['chapters'])}, –ß–∞–Ω–∫–æ–≤: {len(chunks)}")
        
        return {
            'structure': self.doc_structure,
            'chunks_count': len(chunks),
            'chapters_count': len(self.doc_structure['chapters'])
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        import traceback
        traceback.print_exc()
        raise